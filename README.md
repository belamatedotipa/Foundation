# Foundation - Spotify + text to image AI

You are probably familiar with the Spotify service. It's the biggest music streaming platform in the world and they have a nice and well documented API used in this app.

The app consists of the following parts:
- ‚úÖ Login Screen 
    - performs login with Spotify, 
    - USPs/reasons to connect are shown
    - App authenticates through the Spotify app
    - If Spotify is not installed goes via the AppStore.
- ‚úÖ Search Screen - User can search for songs, consists of a search bar and a list of results (songs)
- ‚úÖ Track page - consisting of the name, image generated by the text to image AI by using the song's title as a prompt

### Third party libraries
-  ‚úÖ  Spotify SDK.
    -- This not very well maintained, but it still works with some workarounds. I have used the SDK for the login, with an additional token call, because it's not up to date. 
- ‚úÖ    Alamofire
- ‚úÖ    Lottie
- ‚úÖ    PopupView

API reference:
- [Search](https://beta.developer.spotify.com/documentation/web-api/reference/search/search/)
- [Tracks](https://beta.developer.spotify.com/documentation/web-api/reference/tracks/)

## Additional features 
- ‚úÖ  On the track page, it features a Text to image drawing based on the song‚Äôs title that you just selected, using it as a prompt.
https://replicate.com/pixray/text2image
-- There is a version you can run locally. Docker is not liking the instructions, to keep time in hand I do a simulation for 3 songs based on actual predictions for the songs. 
-- For other song it shows under construction animation and you can easily navigate to the songs that are implemented from the track page.
    -- Even the App Icon was designed by the AI for the Spotify AI prompt.
    -- The assets are currently imported as assets, this is obviously not ideal, but for the sake of demonstration it will do.
- ‚úÖ Optimized for both dark and light modes/appearences

### Responsiveness
‚úÖ   a) There should be some kind of loading indicator to tell the user that something is happening. This is done via ProgressView and Lottie animations

### Resilience
- ‚úÖ - The user is informed if an error occurred while fetching data.
--Right now there is a generic, but stylish error message displayed. This could be extended of course, first with one that informs the user about the lack of internet connection. It doesn't make sense on the other hand to show too many very specific messages because it is hard to interpret by the user.

## General notes
- ‚ö†Ô∏è  Needs to be tested on physical device: Simulator doesn't have an AppStore or Spotify installed

You can make the assessment in your own time. We usually give around 1 week for it.

## Possible next steps
    - Use the real Pixray API or other more advanced service for drawing the song titles
    - Share feature with with a share sheet and the drawn image as attachment, possibly with a default text like "Look! I created this with the Foundation app"
    - Extend testing, Although the are existing tests set up, but many more things could be tested (not just the missing test cases), and some of the tests are failing because the mocks are not fully functional.
    - Save login
    -- üöß - If no network is available when a request is due, the app should park the call and perform it as soon as network is back. 
        --I would at a Reachability observer on app start and listeners in the view models. 
    When the user is trying to make a call I would set a flag, the listener can in turn trigger the call when the connection is restored based on the flag.
        --I would also trigger a persistent popup with the listener, that informs the user about the (lack of) connection.
        --I have used the Reachability.swift package in the past for similar purposes, this package supports the way of thinking I explained.
